{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ase.io import read\n",
    "import mpltex\n",
    "from rascal.representations import SphericalInvariants as SOAP\n",
    "from rascal.utils import get_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import PairwiseKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import Bunch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pylode.projection_coeffs import Density_Projection_Calculator as LODE\n",
    "\n",
    "tab10 = plt.rcParams[\"axes.prop_cycle\"].by_key()['color']\n",
    "poly_kernel = PairwiseKernel(metric=\"polynomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_descriptor(X, n_atoms_per_frame):\n",
    "    \"\"\"Take the sum of a descriptor X over all atoms.\"\"\"\n",
    "    structure_idx = np.cumsum([0] + n_atoms_per_frame)\n",
    "    X_sum = np.array([\n",
    "        np.sum(X[structure_idx[i]:structure_idx[i + 1]], axis=0)\n",
    "        for i in range(len(structure_idx) - 1)\n",
    "    ])\n",
    "\n",
    "    return X_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpltex.acs_decorator\n",
    "def plot_rmsd(bunch_obj, method_key, prop_key, fname=None):\n",
    "    fmts = [\"-\", \"--\", \".-\"]\n",
    "\n",
    "    results_obj = bunch_obj[method_key]\n",
    "    method = method_key\n",
    "\n",
    "    fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "    realiziations = results_obj.keys()\n",
    "    handles_test = len(realiziations) * [None]\n",
    "    handles_train = len(realiziations) * [None]\n",
    "\n",
    "    for i, k in enumerate(realiziations):\n",
    "        label_cutoff = fr\"$r_\\mathrm{{{prop_key}}}=\"\n",
    "        label_cutoff += fr\"{results_obj[k][prop_key]}\\,\\mathrm{{\\AA}}$\"\n",
    "\n",
    "        handles_test[i] = ax.plot(r_train_structures,\n",
    "                                  results_obj[k].rmse_test,\n",
    "                                  fmts[i],\n",
    "                                  c=\"k\",\n",
    "                                  label=rf\"Test, {method}, {label_cutoff}\")[0]\n",
    "\n",
    "        handles_train[i] = ax.plot(\n",
    "            r_train_structures,\n",
    "            results_obj[k].rmse_train,\n",
    "            fmts[i],\n",
    "            c=tab10[0],\n",
    "            label=rf\"Train, {method}, {label_cutoff}\")[0]\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xscale(\"log\")\n",
    "\n",
    "    ax.set_xlim(1e1, 2e3)\n",
    "    ax.set_ylim(1e-2, 1e2)\n",
    "\n",
    "    ax.set_xlabel(\"training structures\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "\n",
    "    handles = handles_test + handles_train\n",
    "    labels = [handle.get_label() for handle in handles]\n",
    "    ax.legend(handles,\n",
    "              labels,\n",
    "              ncol=2,\n",
    "              handlelength=1,\n",
    "              frameon=True,\n",
    "              edgecolor=\"None\",\n",
    "              fontsize=7)\n",
    "\n",
    "    if fname is not None:\n",
    "        fig.savefig(fname, transparent=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../datasets/point_charges_Training_set.xyz\"\n",
    "\n",
    "frames = read(input_file, index=':')\n",
    "n_frames = len(frames)\n",
    "n_atoms_per_frame = [len(frame) for frame in frames]\n",
    "\n",
    "# Move atoms in unitcell\n",
    "for frame in frames:\n",
    "    frame.wrap()\n",
    "\n",
    "# extract energy\n",
    "Y = np.array([frame.info[\"energy\"] for frame in frames])\n",
    "\n",
    "# Create an object/dictionary for storing results\n",
    "results = Bunch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split Train and Test data\n",
    "\n",
    "Here only for Y since the X depends on our applied model (SOAP, LODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = 0.75  # factor of the train set picked from the total set\n",
    "n_subsets = 10  # number of subsets picked on a logspace for scaling test\n",
    "\n",
    "f_test = 1 - f_train\n",
    "i_train, i_test = train_test_split(np.arange(n_frames),\n",
    "                                   test_size=f_test,\n",
    "                                   random_state=0)\n",
    "n_train = len(i_train)\n",
    "n_test = len(i_test)\n",
    "\n",
    "# Split frames\n",
    "frames_train = [frames[i] for i in i_train]\n",
    "frames_test = [frames[i] for i in i_test]\n",
    "\n",
    "# Split energies\n",
    "Y_train = Y[i_train]\n",
    "Y_test = Y[i_test]\n",
    "\n",
    "# Generate subsets numbers for training curve\n",
    "exp_max = math.log(n_train, 10)\n",
    "r_train_structures = np.logspace(1, exp_max, num=n_subsets, endpoint=True)\n",
    "r_train_structures = np.round(r_train_structures).astype(int)\n",
    "\n",
    "# Remove doubled values\n",
    "r_train_structures = np.unique(r_train_structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalc = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.soap = Bunch()\n",
    "r_cut = [3, 6, 9]\n",
    "regularisation_soap = 10e-3  # From paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/precomputed_soap_3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         np\u001b[38;5;241m.\u001b[39msave(fname_precomputed, results\u001b[38;5;241m.\u001b[39msoap[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcut\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mX_raw)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         results\u001b[38;5;241m.\u001b[39msoap[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcut\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mX_raw \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_precomputed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m num_features_soap \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39msoap[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr_cut[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mX_raw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/precomputed_soap_3'"
     ]
    }
   ],
   "source": [
    "for cut in r_cut:\n",
    "    results.soap[f\"r_{cut}\"] = Bunch()\n",
    "    results.soap[f\"r_{cut}\"].cut = cut\n",
    "    fname_precomputed = f\"../datasets/precomputed_soap_{cut}\"\n",
    "    hypers_soap = dict(\n",
    "        soap_type='PowerSpectrum',  # nu = 2\n",
    "        max_radial=2,\n",
    "        max_angular=6,\n",
    "        gaussian_sigma_type='Constant',\n",
    "        gaussian_sigma_constant=1.0,\n",
    "        cutoff_smooth_width=0.5)\n",
    "\n",
    "    if recalc:\n",
    "        hypers_soap[\"interaction_cutoff\"] = cut\n",
    "\n",
    "        calculator = SOAP(**hypers_soap)\n",
    "        soap_rep = calculator.transform(frames)\n",
    "        X_raw = soap_rep.get_features(calculator)\n",
    "\n",
    "        results.soap[f\"r_{cut}\"].X_raw = X_raw\n",
    "        np.save(fname_precomputed, results.soap[f\"r_{cut}\"].X_raw)\n",
    "\n",
    "    else:\n",
    "        results.soap[f\"r_{cut}\"].X_raw = np.load(fname_precomputed)\n",
    "\n",
    "num_features_soap = results.soap[f\"r_{r_cut[0]}.npy\"].X_raw.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cut in r_cut:\n",
    "    X = sum_descriptor(results.soap[f\"r_{cut}\"].X_raw, n_atoms_per_frame)\n",
    "    X_train = X[i_train, :]\n",
    "    X_test = X[i_test, :]\n",
    "\n",
    "    rmse_train = np.zeros(len(r_train_structures))\n",
    "    rmse_test = np.zeros(len(r_train_structures))\n",
    "\n",
    "    for i, n_train_structure in enumerate(r_train_structures):\n",
    "        X_train_cur = X_train[:n_train_structure, :]\n",
    "        Y_train_cur = Y_train[:n_train_structure]\n",
    "\n",
    "        gpr = GaussianProcessRegressor(kernel=poly_kernel,\n",
    "                                       random_state=0,\n",
    "                                       alpha=regularisation_soap)\n",
    "        gpr.fit(X_train_cur, Y_train_cur)\n",
    "\n",
    "        Y_train_pred = gpr.predict(X_train_cur)\n",
    "        Y_test_pred = gpr.predict(X_test)\n",
    "\n",
    "        rmse_train[i] = get_score(Y_train_cur, Y_train_pred)[\"RMSE\"]\n",
    "        rmse_test[i] = get_score(Y_test, Y_test_pred)[\"RMSE\"]\n",
    "\n",
    "    results.soap[f\"r_{cut}\"].rmse_train = rmse_train.copy()\n",
    "    results.soap[f\"r_{cut}\"].rmse_test = rmse_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rmsd(results, \"soap\", \"cut\", fname=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LODE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.lode = Bunch()\n",
    "r_smearing = [1, 0.5]  # computational cost scales cubically with 1/smearing\n",
    "regularisation_lode = 10e-6  # From paper\n",
    "species_dict = {'Na': 0, 'Cl': 1}\n",
    "\n",
    "hypers_lode = dict(\n",
    "    max_angular=6,\n",
    "    cutoff_radius=3,\n",
    "    potential_exponent=1,  # currently, only the exponent p=1 is supported\n",
    "    compute_gradients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a LONG time if `recalc=True`!\n",
    "for smearing in r_smearing:\n",
    "    results.soap[f\"r_{smearing}\"] = Bunch()\n",
    "    results.soap[f\"r_{smearing}\"].smearing = smearing\n",
    "    fname_precomputed = f\"../datasets/precomputed_lode_{smearing}\"\n",
    "    hypers_lode = dict(\n",
    "        max_angular=6,\n",
    "        cutoff_radius=3,\n",
    "        potential_exponent=1,  # currently, only the exponent p=1 is supported\n",
    "        compute_gradients=False)\n",
    "\n",
    "    if recalc:\n",
    "        hypers_lode[\"smearing\"] = smearing\n",
    "\n",
    "        calculator = LODE(**hypers_lode)\n",
    "        lode_rep = calculator.transform(frames, species_dict)\n",
    "        X_raw = lode_rep.get_features(calculator)\n",
    "\n",
    "        results.lode[f\"r_{smearing}\"].X_raw = X_raw\n",
    "        np.save(fname_precomputed, results.lode[f\"r_{smearing}\"].X_raw)\n",
    "\n",
    "    else:\n",
    "        results.soap[f\"r_{smearing}\"].X_raw = np.load(fname_precomputed)\n",
    "\n",
    "num_features_lode = results.lode[f\"r_{r_smearing[0]}\"].X_raw.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for smearing in r_smearing:\n",
    "    X = sum_descriptor(results.lode[f\"r_{smearing}\"].X_raw, n_atoms_per_frame)\n",
    "    X_train = X[i_train, :]\n",
    "    X_test = X[i_test, :]\n",
    "\n",
    "    rmse_train = np.zeros(len(r_train_structures))\n",
    "    rmse_test = np.zeros(len(r_train_structures))\n",
    "\n",
    "    for i, n_train_structure in enumerate(r_train_structures):\n",
    "        X_train_cur = X_train[:n_train_structure, :]\n",
    "        Y_train_cur = Y_train[:n_train_structure]\n",
    "\n",
    "        gpr = GaussianProcessRegressor(kernel=poly_kernel,\n",
    "                                       random_state=0,\n",
    "                                       alpha=regularisation_soap)\n",
    "        gpr.fit(X_train_cur, Y_train_cur)\n",
    "\n",
    "        Y_train_pred = gpr.predict(X_train_cur)\n",
    "        Y_test_pred = gpr.predict(X_test)\n",
    "\n",
    "        rmse_train[i] = get_score(Y_train_cur, Y_train_pred)[\"RMSE\"]\n",
    "        rmse_test[i] = get_score(Y_test, Y_test_pred)[\"RMSE\"]\n",
    "\n",
    "    results.lode[f\"r_{smearing}\"].rmse_train = rmse_train.copy()\n",
    "    results.lodee[f\"r_{smearing}\"].rmse_test = rmse_test.copy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
